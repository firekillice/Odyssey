<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Docker、K8S的网络怎么玩的 | 吹风的坚果</title>
    <meta name="generator" content="VuePress 1.9.7">
    
    <meta name="description" content="">
    
    <link rel="preload" href="/assets/css/0.styles.280e3266.css" as="style"><link rel="preload" href="/assets/js/app.598d8060.js" as="script"><link rel="preload" href="/assets/js/2.4f2e74d3.js" as="script"><link rel="preload" href="/assets/js/4.03d8bd1f.js" as="script"><link rel="prefetch" href="/assets/js/10.e79c846a.js"><link rel="prefetch" href="/assets/js/11.d336f07f.js"><link rel="prefetch" href="/assets/js/12.ebf97c23.js"><link rel="prefetch" href="/assets/js/13.f9fc41aa.js"><link rel="prefetch" href="/assets/js/14.1d241cc3.js"><link rel="prefetch" href="/assets/js/15.a81b04cd.js"><link rel="prefetch" href="/assets/js/16.dc1fed68.js"><link rel="prefetch" href="/assets/js/17.d6c05371.js"><link rel="prefetch" href="/assets/js/18.38605d93.js"><link rel="prefetch" href="/assets/js/19.5d79d441.js"><link rel="prefetch" href="/assets/js/20.51402678.js"><link rel="prefetch" href="/assets/js/21.ad3ac771.js"><link rel="prefetch" href="/assets/js/22.85fe41d1.js"><link rel="prefetch" href="/assets/js/23.78ae8a1e.js"><link rel="prefetch" href="/assets/js/24.4c1e0764.js"><link rel="prefetch" href="/assets/js/25.a6336fbd.js"><link rel="prefetch" href="/assets/js/26.7bc87b9c.js"><link rel="prefetch" href="/assets/js/27.a8bcba1d.js"><link rel="prefetch" href="/assets/js/28.2a827e55.js"><link rel="prefetch" href="/assets/js/3.3a22e7ff.js"><link rel="prefetch" href="/assets/js/5.6607a218.js"><link rel="prefetch" href="/assets/js/6.125d6abe.js"><link rel="prefetch" href="/assets/js/7.bb4679bc.js"><link rel="prefetch" href="/assets/js/8.584c5789.js"><link rel="prefetch" href="/assets/js/9.d75e3788.js">
    <link rel="stylesheet" href="/assets/css/0.styles.280e3266.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">吹风的坚果</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/protocol/" class="nav-link">
  协议
</a></div><div class="nav-item"><a href="/book/" class="nav-link">
  读书
</a></div><div class="nav-item"><a href="/insight/" class="nav-link">
  浅见
</a></div><div class="nav-item"><a href="/cncf/" class="nav-link router-link-active">
  云原生
</a></div><div class="nav-item"><a href="/security/" class="nav-link">
  安全
</a></div><div class="nav-item"><a href="/alg/" class="nav-link">
  算法
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/protocol/" class="nav-link">
  协议
</a></div><div class="nav-item"><a href="/book/" class="nav-link">
  读书
</a></div><div class="nav-item"><a href="/insight/" class="nav-link">
  浅见
</a></div><div class="nav-item"><a href="/cncf/" class="nav-link router-link-active">
  云原生
</a></div><div class="nav-item"><a href="/security/" class="nav-link">
  安全
</a></div><div class="nav-item"><a href="/alg/" class="nav-link">
  算法
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>云原生</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/cncf/container-network.html" aria-current="page" class="active sidebar-link">Docker、K8S的网络怎么玩的</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/cncf/container-network.html#利用命令模拟docker网络" class="sidebar-link">利用命令模拟Docker网络</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/cncf/container-network.html#namespace-命名空间" class="sidebar-link">Namespace(命名空间)</a></li><li class="sidebar-sub-header"><a href="/cncf/container-network.html#veth-pair-virtual-ethernet" class="sidebar-link">veth pair(Virtual Ethernet)</a></li></ul></li><li class="sidebar-sub-header"><a href="/cncf/container-network.html#查看docker具体的使用方式" class="sidebar-link">查看Docker具体的使用方式</a></li><li class="sidebar-sub-header"><a href="/cncf/container-network.html#kubernets网络" class="sidebar-link">Kubernets网络</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/cncf/container-network.html#提出问题" class="sidebar-link">提出问题</a></li><li class="sidebar-sub-header"><a href="/cncf/container-network.html#先有一个k8s环境" class="sidebar-link">先有一个K8S环境</a></li><li class="sidebar-sub-header"><a href="/cncf/container-network.html#节点内部通信" class="sidebar-link">节点内部通信</a></li><li class="sidebar-sub-header"><a href="/cncf/container-network.html#跨节点通信" class="sidebar-link">跨节点通信</a></li><li class="sidebar-sub-header"><a href="/cncf/container-network.html#service的实现" class="sidebar-link">Service的实现</a></li><li class="sidebar-sub-header"><a href="/cncf/container-network.html#coredns" class="sidebar-link">Coredns</a></li></ul></li><li class="sidebar-sub-header"><a href="/cncf/container-network.html#总结" class="sidebar-link">总结</a></li><li class="sidebar-sub-header"><a href="/cncf/container-network.html#引用" class="sidebar-link">引用</a></li></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="docker、k8s的网络怎么玩的"><a href="#docker、k8s的网络怎么玩的" class="header-anchor">#</a> Docker、K8S的网络怎么玩的</h1> <p>能看到这篇文章，那你肯定是对容器、docker、k8s等有兴趣，那么本文将非常适合你，本文从网络这个角度去分析容器化网络用到的一些技术与内容，先使用命令模拟一个docker的环境，了解一下容器化网络的一些基础内容，然后深入分析k8s中容器通信以及Service的各种实现方式，最后简述一下dns的一些使用。希望对你有所帮助，文章较长，最好关注收藏，后面会推出k8s的后续的相关内容。</p> <h2 id="利用命令模拟docker网络"><a href="#利用命令模拟docker网络" class="header-anchor">#</a> 利用命令模拟Docker网络</h2> <h3 id="namespace-命名空间"><a href="#namespace-命名空间" class="header-anchor">#</a> Namespace(命名空间)</h3> <p>在 Linux 容器化技术中，namespace（命名空间）是实现进程隔离的核心技术之一，容器化需要使用 namespace 来提供不同容器之间的隔离和资源独立。<a href="https://en.wikipedia.org/wiki/Linux_namespaces" target="_blank" rel="noopener noreferrer">wiki<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。Linux支持多种命名空间，本文重点关注网络部分，关于网络命名空间有以下描述</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>Network namespaces virtualize the network stack. On creation, a network namespace contains only a loopback interface. Each network interface (physical or virtual) is present in exactly 1 namespace and can be moved between namespaces.
Each namespace will have a private set of IP addresses, its own routing table, socket listing, connection tracking table, firewall, and other network-related resources.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>每个网络命名空间，都有单独的IP地址，路由表，socket列表，防火墙等。</p> <ul><li>网络命名空间使用命令</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>NS_NAME=MyNS
ip netns add $NS_NAME        //添加一个网络命名空间
ip netns list                //查看命名空间
ip netns delete $NS_NAME    //删除命名空间
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>有了命名空间之后，他们是隔离了，如果他们之间需要通信怎么办呢？好办，拉根线呗，接在一起。</p> <h3 id="veth-pair-virtual-ethernet"><a href="#veth-pair-virtual-ethernet" class="header-anchor">#</a> veth pair(Virtual Ethernet)</h3> <p>先创建了namespace进行隔离，在用veth pair进行连接，有点<code>像linux的pipe,将两个进程连接在一起</code>。pair意味着它有两端，我个人觉得可以认为是<code>两头各有一张网卡的网线</code>。下面我们使用veth和namespace来模拟。</p> <h4 id="使用vnet直接连通两个命名空间"><a href="#使用vnet直接连通两个命名空间" class="header-anchor">#</a> 使用vnet直接连通两个命名空间</h4> <ul><li>添加一个veth，一头名字是veth0，一头是veth1，先看一下图<img src="/assets/img/docker-1-v-1.31ca29cf.png" alt="docker-1-v-1"></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>&gt; ip link add veth0 type veth peer name veth1 

&gt; ip link list 查看
...
141: veth1@veth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 2a:49:05:d5:83:0c brd ff:ff:ff:ff:ff:ff
142: veth0@veth1: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 9e:c1:82:e5:e3:10 brd ff:ff:ff:ff:ff:ff
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>请注意此时会多了两个网口，一个名为veth1@veth0，一个名为veth0@veth1，状态都为DOWN</p> <ul><li>创建两个namespace</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>ip netns add my-ns
ip netns add my-ns-peer
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><ul><li>分别将veth的两头放置到不同的namespace下</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>ip link set veth0 netns my-ns
ip link set veth1 netns my-ns-peer
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>查看一下：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>&gt; ip netns exec my-ns ip link list 
142: veth0@if141: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 9e:c1:82:e5:e3:10 brd ff:ff:ff:ff:ff:ff link-netnsid 1

&gt; ip netns exec my-ns-peer ip link list 
141: veth1@if142: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 2a:49:05:d5:83:0c brd ff:ff:ff:ff:ff:ff link-netnsid 0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>这个时候两个网卡的状态还是为DOWN</p> <ul><li>设置IP地址并激活网卡</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>设置veth0
ip netns exec my-ns ip addr add 10.10.1.1/24 dev veth0
ip netns exec my-ns ip link set dev veth0 up

设置veth1
ip netns exec my-ns-peer ip addr add 10.10.1.2/24 dev veth1
ip netns exec my-ns-peer ip link set dev veth1 up
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>查看一下：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>&gt; ip netns exec my-ns ifconfig
veth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 10.10.1.1  netmask 255.255.255.0  broadcast 0.0.0.0
        inet6 fe80::9cc1:82ff:fee5:e310  prefixlen 64  scopeid 0x20&lt;link&gt;
        ether 9e:c1:82:e5:e3:10  txqueuelen 1000  (Ethernet)
        RX packets 8  bytes 648 (648.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 8  bytes 648 (648.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

&gt; ip netns exec my-ns-peer ifconfig
veth1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 10.10.1.2  netmask 255.255.255.0  broadcast 0.0.0.0
        inet6 fe80::2849:5ff:fed5:830c  prefixlen 64  scopeid 0x20&lt;link&gt;
        ether 2a:49:05:d5:83:0c  txqueuelen 1000  (Ethernet)
        RX packets 8  bytes 648 (648.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 8  bytes 648 (648.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>两张网卡都已激活并正确的分配了IP地址</p> <ul><li>测试一下连通性</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>从my-ns空间ping my-ns-peer
&gt; ip netns exec my-ns ping 10.10.1.2
64 bytes from 10.10.1.2: icmp_seq=1 ttl=64 time=0.050 ms

从my-ns-peer空间ping my-ns
&gt; ip netns exec my-ns-peer ping 10.10.1.1
64 bytes from 10.10.1.1: icmp_seq=1 ttl=64 time=0.034 ms
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>这个时候我们就使用一根线将两个namespace连通了。</p> <p>但是这个时候外部是无法访问10.10.1.1和10.10.1.2的</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>&gt; ping 10.10.1.1
2 packets transmitted, 0 received, 100% packet loss, time 1000ms
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><ul><li>如果你已经完全操作结束了，可以使用如下命令清理现场</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>ip netns exec my-ns ip link list
ip netns delete my-ns
ip netns delete my-ns-peer
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><h4 id="多个namespace以及与宿主机通信"><a href="#多个namespace以及与宿主机通信" class="header-anchor">#</a> 多个namespace以及与宿主机通信</h4> <p>使用veth直连的方式只能解决两个namespace之间通信的问题，但是如果有多个namespace相互通信，veth就力不从心了。我们需要使用新的虚拟设备，虚拟网桥，下面我们做一下连通的实验,
先看一下总体的结构图，<img src="/assets/img/docker-host-3-ns.ab4f40e9.png" alt="docker-host-3-ns"></p> <ul><li>创建一个虚拟网桥</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>ip link add br0 type bridge
ip link set br0 up
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><ul><li>创建3个命名空间</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>ip netns add my-ns-0
ip netns add my-ns-1
ip netns add my-ns-2
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><ul><li>创建三个veth</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>ip link add veth0 type veth peer name br-veth0
ip link add veth1 type veth peer name br-veth1
ip link add veth2 type veth peer name br-veth2
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><ul><li>将三个虚拟设备的一端放入分别的命名空间</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>ip link set veth0 netns my-ns-0
ip link set veth1 netns my-ns-1
ip link set veth2 netns my-ns-2
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><ul><li>将三个虚拟设备另一端连接到虚拟网桥并启动</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>ip link set br-veth0 master br0
ip link set br-veth0 up

ip link set br-veth1 master br0
ip link set br-veth1 up

ip link set br-veth2 master br0
ip link set br-veth2 up
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><ul><li>设置不同命名空间中设备的ip地址</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>ip netns exec my-ns-0 ip addr add 10.1.1.2/24 dev veth0
ip netns exec my-ns-0 ip link set veth0 up

ip netns exec my-ns-1 ip addr add 10.1.1.3/24 dev veth1
ip netns exec my-ns-1 ip link set veth1 up

ip netns exec my-ns-2 ip addr add 10.1.1.4/24 dev veth2
ip netns exec my-ns-2 ip link set veth2 up
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><ul><li>测试连通性，如果没有错误的话，这三个命名空间就连接到一起了，但是宿主机无法与三个命名空间通信，因为网桥目前没有IP地址</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>ip netns exec my-ns-0 ping 10.1.1.3
ip netns exec my-ns-1 ping 10.1.1.4
ip netns exec my-ns-2 ping 10.1.1.2
&gt; ping 10.1.1.3 
2 packets transmitted, 0 received, 100% packet loss, time 1000ms
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><ul><li>给网桥设置IP地址</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>ip addr add ip 10.1.1.1/24 dev br0
&gt; ifconfig 
br0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 10.1.1.1  netmask 255.255.255.0  broadcast 0.0.0.0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><ul><li>测试联通性，如果没有错误的话，现在宿主机就和三个命名空间连通了</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>从my-ns-0测试到网桥的连通性
&gt; ip netns exec my-ns-0 ping 10.1.1.1 
  PING 10.1.1.1 (10.1.1.1) 56(84) bytes of data.
  64 bytes from 10.1.1.1: icmp_seq=1 ttl=64 time=0.060 ms

从宿主机测试到my-ns-0的连通性
&gt; ping 10.1.1.2
  PING 10.1.1.2 (10.1.1.2) 56(84) bytes of data.
  64 bytes from 10.1.1.2: icmp_seq=1 ttl=64 time=0.057 ms

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><ul><li>这个时候虽然之间连通了，但是还是无法访问外网，继续搞，添加网桥的192.168.1.1作为网关</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>ip netns exec my-ns-0 route add default gw 10.1.1.1
ip netns exec my-ns-1 route add default gw 10.1.1.1
ip netns exec my-ns-2 route add default gw 10.1.1.1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><ul><li>还是不通，因为我的宿主机的IP是10.10.30.71，这个时候数据包能够出去，但是无法回来(可以在br0和eth0抓包观察)，因为10.1.1.0是个私有网段，不在上层网络的路由规则中，这个需要加一条,这里有个iptables的插图，理解POSTROUTING的位置（iptables如果不太熟悉可以先简单记住就是<code>在数据包离开主机之前所有来自10.1.1.0/24的数据ip都需要被修改</code>）</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>iptables -t nat -A POSTROUTING -s 10.1.1.0/24 -d 0.0.0.0/0 -j MASQUERADE
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这个时候我在my-ns-0中执行<code>ip netns exec my-ns-0 ping 8.8.8.8</code>，分别在三个网口上进行转包，距离由近到远</p> <ul><li>在veth的对端抓包</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>&gt; tcpdump -n -nn -i br-veth0 dst host 8.8.8.8    
  tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
  listening on br0, link-type EN10MB (Ethernet), capture size 262144 bytes
  20:13:00.965858 IP 10.1.1.2 &gt; 8.8.8.8: ICMP echo request, id 23016, seq 38, length 64
  20:13:01.966316 IP 10.1.1.2 &gt; 8.8.8.8: ICMP echo request, id 23016, seq 39, length 64
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><ul><li>在网桥上抓包，源IP保持不变</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>&gt; tcpdump -n -nn -i br0 dst host 8.8.8.8    
  tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
  listening on br0, link-type EN10MB (Ethernet), capture size 262144 bytes
  20:13:00.965858 IP 10.1.1.2 &gt; 8.8.8.8: ICMP echo request, id 23016, seq 38, length 64
  20:13:01.966316 IP 10.1.1.2 &gt; 8.8.8.8: ICMP echo request, id 23016, seq 39, length 64
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><ul><li>宿主机eth0转抓包,可以看到所有的源IP都转为了宿主机的10.10.30.71</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>&gt; tcpdump -n -nn -i eth0 dst host 8.8.8.8
  tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
  listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes
  20:12:32.956569 IP 10.10.30.71 &gt; 8.8.8.8: ICMP echo request, id 23016, seq 10, length 64
  20:12:33.956712 IP 10.10.30.71 &gt; 8.8.8.8: ICMP echo request, id 23016, seq 11, length 64
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>这样就可以连通外网了。</p> <ul><li>最后一步，端口映射，在my-ns-0空间启动一个端口8000的http服务</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>&gt; ip netns exec my-ns-0   python3 -m http.server 8000
&gt; netstat -an | grep 8000 在宿主机上无法看到
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>因为网络是独立的，需要添加转发规则</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>iptables -t nat -A PREROUTING -p tcp --dport 9090 -j DNAT --to-destination 10.1.1.2:8000
iptables -t nat -nvL | grep 9090 查看规则是否添加成功
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>将宿主机上对9090端口的请求改为对10.1.1.2:8000请求，这个时候访问宿主机的http://10.10.30.71:9090(我的机器为10.10.30.71)就可以正常访问python启动的http服务了</p> <ul><li>至此为止，我们就将多个namespace与宿主机连通了，能够互相访问，能够外网访问，能够启动端口对外提供服务。docker的网络的默认网桥模式基于以上原理实现的。</li></ul> <h2 id="查看docker具体的使用方式"><a href="#查看docker具体的使用方式" class="header-anchor">#</a> 查看Docker具体的使用方式</h2> <ul><li>以我的机器为例子，我有一个<code>docker0</code>的网桥，ip地址为172.17.0.1</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>&gt; ifconfig
docker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255
&gt; brctl show
  bridge name     bridge id               STP enabled     interfaces
  docker0         8000.0242b38b3dee       no              vetha0fa6b7
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><ul><li>在一个containerId为9f740ba587fe的容器，执行<code>docker exec -it 9f740ba587fe /bin/sh</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>&gt; ifconfig
eth0      Link encap:Ethernet  HWaddr 02:42:AC:11:00:03  
          inet addr:172.17.0.3  Bcast:172.17.255.255  Mask:255.255.0.0
&gt; route -n
/ # route -n
  Kernel IP routing table
  Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
  0.0.0.0         172.17.0.1      0.0.0.0         UG    0      0        0 eth0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>ip地址为172.17.0.3，网关为172.17.0.1就是上面的docker0网桥的地址</p> <ul><li>查看容器的命名空间</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>lsns | grep `docker inspect -f {{.State.Pid}} 9f740ba587fe`
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>查看地址伪装</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>&gt; iptables -t nat -nvL  | grep 172.17
0     0 MASQUERADE  all  --  *      *       172.17.0.0/16        0.0.0.0/0   
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><ul><li>查看端口映射，该容器将8080端口映射到了容器中的80端口</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>&gt; docker ps 
9f740ba587fe   tranglolab/kafka-connector-board   &quot;nginx -g 'daemon of…&quot;   7 weeks ago   Up 7 weeks   0.0.0.0:8080-&gt;80/tcp, :::8080-&gt;80/tcp   hopeful_brattain
&gt; iptables -t nat -nvL  | grep 8080
4   208 DNAT       tcp  --  !docker0 *       0.0.0.0/0            0.0.0.0/0            tcp dpt:8080 to:172.17.0.3:80
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>172.17.0.3就是上面看到的容器的ip地址，将8080端口映射到了172.17.0.3:80端口</p> <ul><li>我们利用namespace、veth、iptables三者集合起来，就模拟和验证了docker网络组织的基本方式，这个其实是docker的网桥模式，还有host网络模式，这里就不展开了。</li></ul> <h2 id="kubernets网络"><a href="#kubernets网络" class="header-anchor">#</a> Kubernets网络</h2> <p>Docker的网络相对比较简单，但是这个是理解k8s网络的基础。K8S的网络可以说涉及到了计算机网络的各个方面，IP分配、节点连通、网关、DNS，路由等等，我们将k8s的容器网络与真实世界网络对比理解，也许效果更好。<img src="/assets/img/TwoStickmenTalking.f890b646.gif" alt="realvsk8s"></p> <h3 id="提出问题"><a href="#提出问题" class="header-anchor">#</a> 提出问题</h3> <p><a href="https://kubernetes.io/zh-cn/docs/concepts/cluster-administration/networking/" target="_blank" rel="noopener noreferrer">K8S的官网<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>上提出了集群网络系统的四个问题</p> <ul><li>高度耦合的容器间通信：这个已经被 Pod 和 localhost 通信解决了。</li> <li>Pod 间通信</li> <li>Pod 与 Service 间通信</li> <li>外部与 Service 间通信
也给出了<a href="https://kubernetes.io/zh-cn/docs/concepts/services-networking/" target="_blank" rel="noopener noreferrer">网络模型<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <h3 id="先有一个k8s环境"><a href="#先有一个k8s环境" class="header-anchor">#</a> 先有一个K8S环境</h3> <ul><li>我部署了一个K0S，用的版本是k0s-v1.32.1+k0s.0-amd64，部署了3个机器，一个controller，两个worker，结构和IP如下
<img src="/assets/img/k0s-physical-layout.24206e80.png" alt="k0s-physical-layout"></li> <li>网络通信这块的内容比较多，我们分为两种情况讨论，节点内通信和跨节点通信。</li></ul> <h3 id="节点内部通信"><a href="#节点内部通信" class="header-anchor">#</a> 节点内部通信</h3> <h4 id="我们先看看单个pod网络连接和设备对应情况"><a href="#我们先看看单个pod网络连接和设备对应情况" class="header-anchor">#</a> 我们先看看单个Pod网络连接和设备对应情况</h4> <ul><li>启动一个BusyBox</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>apiVersion: v1
kind: Pod
metadata:
  name: busybox-pod
  labels:
    app: busybox-app
spec:
  containers:
  - name: busybox-container
    image: capnexus-registry.capstonedev.cn/library/busybox:latest
    command: [&quot;sh&quot;, &quot;-c&quot;, &quot;while true; do echo 'Busybox is running'; sleep 10; done&quot;]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>运行得到以下信息的Pod</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>Name: busybox-pod
Namespace: default
Running Node: 192-168-40-35
Pod IP: 10.244.0.28
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>所以busybox-podd的宿主机为<code>192.168.40.35</code></p> <ul><li>在<code>192.168.40.35</code>执行： <code>kubectrl exec -n default busybox-pod -i -t -- ip link show</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>  ...
  2: eth0@if8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue 
      link/ether 76:7e:4c:45:05:77 brd ff:ff:ff:ff:ff:ff
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这里看到eth0@if8</p> <ul><li>在<code>192.168.40.35</code>中执行<code>ip link</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>  ...
  8: veth9750d64d@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master kube-bridge state UP mode DEFAULT group default 
      link/ether 86:bc:22:8a:3d:83 brd ff:ff:ff:ff:ff:ff link-netnsid 3
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>我们看到序号为8的设备是在<code>link-netnsid 3</code>中</p> <ul><li>在<code>192.168.40.35</code>上执行<code>ip netns</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>  cni-9932435c-22c0-31fc-f51b-ee38cee99b01 (id: 3)
  cni-62099dba-a8a9-3628-d605-1ed794d5b220 (id: 1)
  cni-8fffbde1-4bf4-d317-fea0-16b91a20d73d (id: 2)
  cni-a0edeb62-71d7-f5e2-230b-2fea58467baa (id: 0)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>找到id：3的网络命名空间</p> <ul><li>在<code>192.168.40.35</code>执行</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>ip netns exec cni-9932435c-22c0-31fc-f51b-ee38cee99b01 ip address
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这个时候我们应该能够看到和<code>kubectrl exec -n default busybox-pod -i -t -- ip link show</code>一样的结果</p> <ul><li>在<code>192.168.40.35</code>上执行<code>brctl show</code>，此处需要安装brctrl工具<code>yum install bridge-utils</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>kube-bridge             8000.cac33cb58442       no              veth01e0bb28
                                                        veth70c81cf8
                                                        veth9750d64d
                                                        vetha2ac3686
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><ul><li>我们可以看到有4个设备连接到了kube-bridge，然后从<code>ip link</code>中都可以看到kube-bridge和连接到四个interface的设备</li> <li>总结：busybox-pod通过veth veth9750d64d连接到kube-bridge上，在 cni-9932435c-22c0-31fc-f51b-ee38cee99b01这个网络命名空间中，使用的是IP是<code>10.244.0.28</code></li></ul> <h5 id="pod内部的容器通信"><a href="#pod内部的容器通信" class="header-anchor">#</a> Pod内部的容器通信</h5> <ul><li>安装一个包含两个容器的Pod，使用busybox和node，其中Node启动了一个3000的HTTP端口</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>apiVersion: v1
kind: Pod
metadata:
  name: multi-container-pod
  labels:
    app: multi-container-example
spec:
  containers:
  - name: busybox-container
    image: busybox:latest
    command: [&quot;sh&quot;, &quot;-c&quot;, &quot;while true; do echo 'Busybox is running'; sleep 10; done&quot;]
  - name: node-container
    image: node:latest
    command: [&quot;node&quot;, &quot;-e&quot;, &quot;require('http').createServer((req, res) =&gt; { res.end('Hello from Docker!'); }).listen(3000, '0.0.0.0', () =&gt; { conso
le.log('Server running at http://0.0.0.0:3000'); });&quot;]
    ports:
    - containerPort: 3000
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><ul><li>使用这个命令<code>kubectrl exec -n default multi-container-pod -c busybox-container -- telnet 127.0.0.1 3000</code>，从<code>busybox-container</code>连接3000端口输出为</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>Connected to 127.0.0.1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>说明在容器内部使用127.0.0.1即可通信</p> <h4 id="相同机器上不同pod的容器通信"><a href="#相同机器上不同pod的容器通信" class="header-anchor">#</a> 相同机器上不同Pod的容器通信</h4> <ul><li>部署一个新的busybox，名字为<code>busybox-pod2</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>apiVersion: v1
kind: Pod
metadata:
  name: busybox-pod2
  labels:
    app: busybox-app
spec:
  containers:
  - name: busybox-container
    image: capnexus-registry.capstonedev.cn/library/busybox:latest
    command: [&quot;sh&quot;, &quot;-c&quot;, &quot;while true; do echo 'Busybox is running'; sleep 10; done&quot;]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>和前文中的<code>busybox-pod</code>一样都运行在宿主机<code>192.168.40.35</code>下（运气好），IP地址为<code>10.244.0.29</code>，形成了如下结构
<img src="/assets/img/two-pod-in-same-node.ae3551fb.png" alt="two-pod-in-same-node"></p> <ul><li>两个Pod的IP分别为<code>10.244.0.28</code>和<code>10.244.0.29</code>，他们通过<code>kube-bridge</code>在同一个网段中，可以通过ARP协议获取MAC地址进行通信。使用<code>kubectrl exec -n default busybox-pod -i -t -- arp -an</code>查看会发现</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>? (10.244.0.29) at 82:bf:18:88:ca:89 [ether]  on eth0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>我们运行<code>kubectrl exec -n default busybox-pod -i -t -- ping 10.244.0.29</code>的同时启动<code>tcpdump -i kube-bridge -ent arp</code>可以看到不定时的</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>76:7e:4c:45:05:77 &gt; 82:bf:18:88:ca:89, ethertype ARP (0x0806), length 42: Request who-has 10.244.0.29 tell 10.244.0.28, length 28
82:bf:18:88:ca:89 &gt; 76:7e:4c:45:05:77, ethertype ARP (0x0806), length 42: Request who-has 10.244.0.28 tell 10.244.0.29, length 28 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>而使用<code>tcpdump -i eth0 -ent arp</code>是看不到，也就是说ARP协议包都是通过kube-bridge进行的。</p> <h4 id="分析不同机器的pod的容器通信到底存在什么问题"><a href="#分析不同机器的pod的容器通信到底存在什么问题" class="header-anchor">#</a> 分析不同机器的Pod的容器通信到底存在什么问题</h4> <p>我们创建了一个名为<code>multi-container-pod</code>的Pod，它运行在<code>192.168.40.36</code>，我们尝试连通<code>multi-container-pod</code>和<code>busybox-pod</code>(运行在192.168.40.35)，<code>multi-container-pod</code>的IP为<code>10.244.1.4</code>，<code>busybox-pod</code>的IP为<code>10.244.0.28</code>。如下结构
<img src="/assets/img/two-pods-in-diff-node.b9426b8c.png" alt="two-pods-in-diff-node"></p> <ul><li>执行<code>kubectrl exec -n default busybox-pod -i -t -- ip address</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>...
    inet 10.244.0.28/24 brd 10.244.0.255 scope global eth0
...
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>可以看到它<code>10.244.0.28</code>是24位的地址，和<code>10.244.1.4</code>不在同一个网段,这个时候我们就用到了以下信息<code>kubectrl exec -n default busybox-pod -i -t -- route -n</code></p> <div class="language- line-numbers-mode"><pre class="language-text"><code>Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         10.244.0.1      0.0.0.0         UG    0      0        0 eth0
10.244.0.0      0.0.0.0         255.255.255.0   U     0      0        0 eth0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>它的网关是<code>10.244.0.1</code>，这个时候<code>10.244.0.28</code>就会发送ARP广播获取<code>10.244.0.1</code>的MAC地址，获取MAC地址后就会将消息发送给<code>10.244.0.1</code>让它来处理。
这个时候就涉及到一个问题，一个A网段中间隔着B网段，将数据发往C网段
<img src="/assets/img/problem-off-cross-net.b9cca0cc.png" alt="problem-off-cross-net"></p> <ul><li>看到这张图，如果你给出解决方案，你的方案是什么呢？这个地方就可以玩很多的花样，下面我们引出跨节点通信。</li></ul> <h3 id="跨节点通信"><a href="#跨节点通信" class="header-anchor">#</a> 跨节点通信</h3> <p>在这里我们稍作挺留，这块内容涉及到了一些知识点，不知道读者你是否已经掌握，包括但不限于ipables、BGP、UDP、IPVS、VXlan、dns等。下面我用几个小节对一些内容做一下预热。</p> <h4 id="热身"><a href="#热身" class="header-anchor">#</a> 热身</h4> <h5 id="vxlan-virtual-extensible-local-area-network虚拟可扩展的局域网-的使用实验"><a href="#vxlan-virtual-extensible-local-area-network虚拟可扩展的局域网-的使用实验" class="header-anchor">#</a> VXLAN(Virtual eXtensible Local Area Network虚拟可扩展的局域网)的使用实验</h5> <div class="language- line-numbers-mode"><pre class="language-text"><code>Linux 内核从Linux 3.7 版本开始支持VXLAN，到了内核3.12 版本对VXLAN 的支持已经完备，支持单播和组播，IPv4 和IPv6
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li><strong>HostA</strong>: 192.168.40.37  <strong>HostB</strong>: 192.168.40.38</li> <li>在<strong>HostA</strong>执行</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>ip link add vxlan1 type vxlan id 1 remote 192.168.40.38 dstport 4789 dev eth0
ip link set vxlan1 up
ip addr add 10.0.0.20/24 dev vxlan1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>我们从<code>add</code>命令上可以看出，vxlan就是做了一个<code>隧道</code>将远端与本地连接起来，相当于<code>灵魂链接</code>。</p> <ul><li>在<strong>HostB</strong>执行</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>ip link add vxlan1 type vxlan id 1 remote 192.168.40.37 dstport 4789 dev eth0
ip link set vxlan1 up
ip addr add 10.0.0.21/24 dev vxlan1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><ul><li>任意主机查看路由<code>route -n</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.1.1     0.0.0.0         UG    100    0        0 eth0
10.0.0.0        0.0.0.0         255.255.255.0   U     0      0        0 vxlan1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>去往10.0.0.0/24都去vxlan1口了</p> <ul><li>任意主机查看端口 <code>netstat -na | grep 4789</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>udp        0      0 0.0.0.0:4789            0.0.0.0:*  
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>自动启动了一个4789端口</p> <ul><li>在<strong>HostB</strong>执行<code>ping 10.0.0.20</code>，在任意主机上执行tcpdump</li> <li>在vxlan1网口抓包，<code>tcpdump -i vxlan1 -n -nn</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>00:03:03.141360 IP 10.0.0.21 &gt; 10.0.0.20: ICMP echo request, id 3012, seq 34, length 64
00:03:03.141408 IP 10.0.0.20 &gt; 10.0.0.21: ICMP echo reply, id 3012, seq 34, length 64
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><ul><li>在eth0网口抓包<code>tcpdump -i eth0 -n -nn host 192.168.40.38</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>  00:03:31.141298 IP 192.168.40.38.38491 &gt; 192.168.40.37.4789: VXLAN, flags [I] (0x08), vni 1
  IP 10.0.0.21 &gt; 10.0.0.20: ICMP echo request, id 3012, seq 62, length 64
  00:03:31.141369 IP 192.168.40.37.40535 &gt; 192.168.40.38.4789: VXLAN, flags [I] (0x08), vni 1
  IP 10.0.0.20 &gt; 10.0.0.21: ICMP echo reply, id 3012, seq 62, length 64
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><ul><li>在eth0传输的vxlan的vni为1的数据，可以看到到vxlan1的数据已经是纯粹的ICMP的数据了。</li> <li>可见是，内核的vxlan模块会处理UDP的数据，去除UDP的包头，然后就查询路由表直接路由到vxlan1这个网口了</li> <li>这个协议名字看起来是vlan，其实是一种overlay，不过有一个vni id</li></ul> <h5 id="ipvs-ip-virtual-server"><a href="#ipvs-ip-virtual-server" class="header-anchor">#</a> IPVS(IP Virtual Server)</h5> <div class="language- line-numbers-mode"><pre class="language-text"><code>linux 内核2.4.x开始支持
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>查看是否启用，如果显示如下，表示启用了，<code>lsmod | grep ip_vs</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>ip_vs_rr               16384  98
ip_vs                 184320  100 ip_vs_rr
nf_conntrack          176128  5 xt_conntrack,nf_nat,nf_conntrack_netlink,xt_MASQUERADE,ip_vs
nf_defrag_ipv6         24576  2 nf_conntrack,ip_vs
libcrc32c              16384  4 nf_conntrack,nf_nat,nf_tables,ip_vs
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><ul><li>from WIKI</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>IPVS (IP Virtual Server) implements transport-layer load balancing, usually called Layer 4 LAN switching, as part of the Linux kernel. It's configured via the user-space utility ipvsadm(8) tool.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>可见ipvs实现的是传输层的负载均衡，通过用户空间的ipvsadm来管理内核态的映射关系。</p> <ul><li>说白了，它可以将tcp/udp协议根据负载均衡的策略分发到不同的目标节点上，也就是内核中自带了一个负载均衡器。</li></ul> <h5 id="iptables"><a href="#iptables" class="header-anchor">#</a> iptables</h5> <ul><li>iptables是基于linux的netfilter框架开发的</li> <li>netfilter提供了5个hook点，其实就是在网络数据包处理的关键位置可以嵌入规则进行处理。
<ul><li>NF_IP_PRE_ROUTING: 进入路由之前</li> <li>NF_IP_LOCAL_IN: 进入本地协议栈之前</li> <li>NF_IP_FORWARD: 确认被转发但是还没有出去</li> <li>NF_IP_LOCAL_OUT: 本地确认出去</li> <li>NF_IP_POST_ROUTING: 本地出去与转发
他们的关系如下,<img src="/assets/img/netfillter-hook.5ffbd833.png" alt="netfillter-hook">，不同的数据包走不通的路径。</li></ul></li> <li>iptables包含多张表，我们使用<code>iptables -nvL</code>默认查看的filter表，如果查看nat表，使用<code>iptables -nvL -t nat</code></li> <li>每张表中都包含Chain，有多种类型的Chain
<ul><li>PREROUTING</li> <li>INPUT</li> <li>FORWARD</li> <li>OUTPUT</li> <li>POSTROUTING</li> <li>自定义Chain
自定义的Chain将处理行为进行封装，从程序员的理解就是<code>这就是定义了一个iptables方法</code>。</li></ul></li> <li>所以hook、chain、table到底有啥关系呢?
<ul><li>在数据包进入hook点之后，会按照如图所示的方式穿越不同的表的不同的chain，表的优先级: raw&gt;mangle&gt;nat&gt;filter，在每个chain内，规则是顺次执行的。
<img src="/assets/img/iptables-chain-hook-rel.d01cd654.png" alt="iptables-chain-hook-rel"></li></ul></li></ul> <h5 id="bgp-border-gateway-protocol-协议"><a href="#bgp-border-gateway-protocol-协议" class="header-anchor">#</a> BGP(Border Gateway Protocol)协议</h5> <ul><li>from wiki</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>Border Gateway Protocol (BGP) is a standardized exterior gateway protocol designed to exchange routing and reachability information among autonomous systems (AS) on the Internet.[2] BGP is classified as a path-vector routing protocol,[3] and it makes routing decisions based on paths, network policies, or rule-sets configured by a network administrator.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>是一个去中心化自治路由协议</li> <li>是一种路径矢量协议，非基于状态的</li> <li>每个路由器会配置一个AS(Autonomous system)编号,AS号是一个16bit的数字，全球共用这60000多个编号。1 – 64511 是全球唯一的，而 64512 – 65535 是可以自用的，类似于私网网段，比如联通的AS号是9800，<a href="http://www.cidr-report.org/as2.0/aggr.html" target="_blank" rel="noopener noreferrer">AS编号查询<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。</li> <li>路由器之间通信使用TCP协议</li> <li>有eBGP与iBGP两种模式</li></ul> <h5 id="tun-tunnel-设备"><a href="#tun-tunnel-设备" class="header-anchor">#</a> Tun(Tunnel)设备</h5> <ul><li>TUN是一种虚拟网络设备，常用于实现网络层的隧道, 它是在操作系统内核中创建的虚拟设备，允许用户空间的程序读取或写入IP数据包，从而可以在不修改实际网络硬件或驱动的情况下实现自定义的网络行为。
<img src="/assets/img/tunnel-work-spot.8254b004.png" alt="tunnel-work-spot">
tunnel设备顾名思义，就是用来做隧道的，通过读文件的方式，实际上用来操作网络协议栈，操作系统太有趣了，这个设备<code>一脚踩在文件层，一脚踩在网络中，头出溜在用户面前</code>。</li> <li>我们常说的VPN底层就是用这种设备和技术实现的。</li></ul> <h5 id="小结"><a href="#小结" class="header-anchor">#</a> 小结</h5> <p>我们通过介绍几个技术项来做个简单的warmup，下面我们趁热打铁看看CNI如何利用这些技术完成通信的。</p> <h4 id="flannel"><a href="#flannel" class="header-anchor">#</a> Flannel</h4> <ul><li><code>Flannel is a simple and easy way to configure a layer 3 network fabric designed for Kubernetes.</code> 它在k8s中部署一个daemonset，在每个节点运行一个<code>flanneld</code>进程来实现其功能。</li></ul> <h5 id="vxlan模式-默认模式"><a href="#vxlan模式-默认模式" class="header-anchor">#</a> VXLAN模式，默认模式</h5> <ul><li>我又部署了一个cni为flannel的k8s集群，我们查看一下相关信息。</li> <li>配置文件</li></ul> <div class="language-net-conf.json line-numbers-mode"><pre class="language-text"><code>{
  &quot;Network&quot;: &quot;10.244.0.0/16&quot;,
  &quot;EnableNFTables&quot;: false,
  &quot;Backend&quot;: {
    &quot;Type&quot;: &quot;vxlan&quot;
  }
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>可以看到该实例基于vxlan模式。
具体例子拓扑如下
<img src="/assets/img/flannel-vxlan.05ec6ac2.png" alt="flannel-vxlan"></p> <ul><li>在192.168.1.37执行 <code>route -n</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>...
10.244.1.0      10.244.1.0      255.255.255.0   UG    0      0        0 flannel.1
...
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>将10.244.1.0/24的数据都路由到设备flannel.1</p> <ul><li>在192.168.1.37使用<code>ip -d link show flannel.1</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>6: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default 
    link/ether b6:dc:09:2b:21:ba brd ff:ff:ff:ff:ff:ff promiscuity 0 
    vxlan id 1 local 192.168.40.37 dev eth0 srcport 0 0 dstport 8472 nolearning ageing 300 noudpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>该设备是一个vxlan设备</p> <ul><li>在192.168.1.37使用<code>bridge fdb show | grep flannel.1</code> 查询fdb(Forwarding Database)转发表，flannel.1链接本地与<code>192.168.40.38</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>  52:ab:e2:49:7c:c8 dev flannel.1 dst 192.168.40.38 self permanent
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>在192.168.1.37查看启动的UDP端口<code>etstat -anp | grep 8472</code><div class="language- line-numbers-mode"><pre class="language-text"><code>udp 0  0  0.0.0.0:8472  0.0.0.0:*   -
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div></li> <li>总之，去往<code>10.244.1.0/24</code>的流量被注入到flannel.1中，进入vxlan的处理逻辑，它根据vlan 1的信息得到源IP为<code>192.168.40.37</code>，目的IP为<code>192.168.40.38</code>，目的端口为<code>8472</code>。</li></ul> <h5 id="udp模式-使用tun设备"><a href="#udp模式-使用tun设备" class="header-anchor">#</a> UDP模式，使用Tun设备</h5> <ul><li>下面我们把上面的flannel改为UDP模式</li></ul> <div class="language-net-conf.json line-numbers-mode"><pre class="language-text"><code>{
  &quot;Network&quot;: &quot;10.244.0.0/16&quot;,
  &quot;EnableNFTables&quot;: false,
  &quot;Backend&quot;: {
    &quot;Type&quot;: &quot;udp&quot;
  }
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><ul><li><p>我们还是在<code>192.168.40.37</code>上执行<code>netstat -anp |grep udp</code>,发现启动了一个UDP端口</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>udp        0      0 192.168.40.37:8285      0.0.0.0:*  2972/flanneld 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div></li> <li><p>多了一个flannel0，我们还是在<code>192.168.40.37</code>执行<code>ip -d link show flannel0</code>，我们发现是tun设备</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>12: flannel0: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1472 qdisc pfifo_fast state UNKNOWN mode DEFAULT group default qlen 500
  link/none  promiscuity 0 
  tun addrgenmode random numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div></li> <li><p><code>route -n</code>,注意，这里是一个10.244.0.0/16的网段</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>10.244.0.0      0.0.0.0         255.255.255.0   U     0      0        0 cni0
10.244.0.0      0.0.0.0         255.255.0.0     U     0      0        0 flannel0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div></li> <li><p>具体的包的流程如下图
<img src="/assets/img/flannel-upd-packet-journal.b2d97453.png" alt="flannel-upd-packet-journal"></p></li> <li><p>开发者在有了接收数据包和发送数据包的权利，那很多事情都可以干了。比如flannel中，容器的数据可以全部通过路由表发送到flanneld后台进程中，这个进程通过查看ip头的目的ip，然后通过etcd中的信息找到该ip所在的host，然后将收到的包封一个UDP包头向host发送出去。然后对端的flanneld收到该UDP包，然后查到目的ip就可以将数据从新发到目的host的tun设备上了。</p></li></ul> <h5 id="host-gw模式"><a href="#host-gw模式" class="header-anchor">#</a> host-gw模式</h5> <ul><li>顾名思义，就是用host做网关，直接寻址，因为网关的MAC地址查询是链路层完成的，所以这种模式适合于二层可达的节点网络。</li> <li>这种模式简单直接 <img src="/assets/img/flannel-host-gw.a24d12b6.png" alt="flannel-host-gw"></li></ul> <h4 id="kube-router"><a href="#kube-router" class="header-anchor">#</a> Kube Router</h4> <h5 id="bgp模式"><a href="#bgp模式" class="header-anchor">#</a> BGP模式</h5> <ul><li>kube router在每个Node上运行一个kube-router进程，他们之间可以进行BGP通信</li> <li>使用的iBGP，即使用相同的AS</li> <li>此处需要下载<a href="https://github.com/osrg/gobgp/releases/tag/v3.5.0" target="_blank" rel="noopener noreferrer">gobgp<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>，在k0s的worker节点上运行一下命令，发现<code>192.168.40.36</code>的邻居为<code>192.168.40.35</code>， <code>192.168.40.35</code>的邻居为<code>192.168.40.36</code>，都在AS 64512中</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>&gt;  ./gobgp  neighbor -u 192.168.40.36
Peer             AS     Up/Down State       |#Received  Accepted
192.168.40.35 64512 1d 08:50:31 Establ      |        1         1

&gt; ./gobgp  neighbor -u 192.168.40.35
Peer             AS     Up/Down State       |#Received  Accepted
192.168.40.36 64512 1d 08:52:03 Establ      |        1         1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><ul><li>还可以使用<code>./gobgp global rib</code>， rib(route information base)，查看详细的路由条目</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>   Network              Next Hop             AS_PATH              Age        Attrs
*&gt; 10.244.0.0/24        192.168.40.35                             00:03:09   [{Origin: i}]
*&gt; 10.244.1.0/24        192.168.40.36                             1d 09:03:40 [{Origin: i} {LocalPref: 100}]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><ul><li>BGP的Peer内部使用179作为TCP的连接端口，在192.168.40.36上执行<code>netstat -anp | grep 179</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>tcp        0      0 192.168.40.36:179       0.0.0.0:*               LISTEN      1718/kube-router    
tcp        0      0 192.168.40.36:50547     192.168.40.35:179       ESTABLISHED 1718/kube-router 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>在192.168.40.35上执行<code>netstat -anp | grep 179</code></p> <div class="language- line-numbers-mode"><pre class="language-text"><code>tcp        0      0 192.168.40.35:179       0.0.0.0:*               LISTEN      1764/kube-router    
tcp        0      0 192.168.40.35:179       192.168.40.36:50547     ESTABLISHED 1764/kube-router 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>在有新的Pod新建的时候使用<code>tcpdump -nn -n port 179 -i eth0</code>有输出</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>18:27:40.907045 IP 192.168.40.36.50547 &gt; 192.168.40.35.179: Flags [P.], seq 1414131710:1414131729, ack 4262141184, win 58, options [nop,nop,TS val 118456940 ecr 121403993], length 19: BGP
18:27:40.907109 IP 192.168.40.35.179 &gt; 192.168.40.36.50547: Flags [.], ack 19, win 57, options [nop,nop,TS val 121418949 ecr 118456940], length 0
18:27:40.907255 IP 192.168.40.35.179 &gt; 192.168.40.36.50547: Flags [P.], seq 1:20, ack 19, win 57, options [nop,nop,TS val 121418949 ecr 118456940], length 19: BGP
18:27:40.946787 IP 192.168.40.36.50547 &gt; 192.168.40.35.179: Flags [.], ack 20, win 58, options [nop,nop,TS val 118456980 ecr 121418949], length 0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><ul><li>我们在<code>192.168.40.35</code>使用<code>route -n</code>查看</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>...
10.244.1.0      192.168.40.36   255.255.255.0   UG    0      0        0 eth0
...
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>在<code>192.168.40.36</code>使用<code>route -n</code></p> <div class="language- line-numbers-mode"><pre class="language-text"><code>...
10.244.0.0      192.168.40.35   255.255.255.0   UG    0      0        0 eth0
...
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>可以看到每个机器上都有一条路由将IP网段与所在的Host关联起来，<code>这一点看起来是flannel的host-gw模式的结果是类似的</code>。</p> <ul><li>可以观看<a href="https://asciinema.org/a/120885" target="_blank" rel="noopener noreferrer">Kube-router: Kubernetes pod-to-pod networking with BGP<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>观察类似的过程</li></ul> <h4 id="calico"><a href="#calico" class="header-anchor">#</a> Calico</h4> <ul><li>有两种模式，一种是IPIP模式，一种是BGP模式，</li> <li>IPIP的实现也是依赖于Tun设备，相对于flannel的UDP模式，不封装传输层的协议，直接封装IP</li> <li>理解了KubeRouter的BGP，这个基本可以脑补出来了</li></ul> <h3 id="service的实现"><a href="#service的实现" class="header-anchor">#</a> Service的实现</h3> <ul><li>kube-proxy执行service的实现，kube-proxy也是deameonset，每个节点一个进程</li> <li>目前有两种实现iptables模式和ipvs模式</li></ul> <h4 id="特别提及的loadbalancer"><a href="#特别提及的loadbalancer" class="header-anchor">#</a> 特别提及的LoadBalancer</h4> <ul><li>该模式就是为了对外暴露服务用的，从这个名字来看，我怀疑这个名字的初衷就是为了对接云厂商的负载均衡器。</li> <li>LoadBalancer 的工作需要搭配第三方的负载均衡器来完成。</li> <li>此处采用的是Metallb来实现内网机器的ip分配</li></ul> <h4 id="metallb"><a href="#metallb" class="header-anchor">#</a> Metallb</h4> <ul><li>为没有负载均衡器的环境而设计</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>MetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>有一个daemonset的speaker，有一个deployment的controller</li> <li>Speaker 则会依据选择的协议进行相应的广播或应答，实现 IP 地址的通信响应</li> <li>Controller 负责监听 Service 变化并分配或回收 IP，即实现IPAM</li> <li>有两种工作模式Lay2和BGP</li></ul> <h5 id="lay2模式"><a href="#lay2模式" class="header-anchor">#</a> Lay2模式</h5> <ul><li>speaker实现了ARP协议</li> <li>speaker会选举一个leader，所有的Loadbalancer的IP地址都在Leader下生成,不能有多个节点同时对外喊话，但是这样会有单节点瓶颈和故障转移慢的问题。</li></ul> <h5 id="bgp模式-2"><a href="#bgp模式-2" class="header-anchor">#</a> BGP模式</h5> <ul><li>不会对分配的IP地址进行ARP的回应</li> <li>是靠网络将数据带到宿主机上的
<img src="/assets/img/metallb-bgp-usage.a96ed107.png" alt="metallb-bgp-usage"></li></ul> <h4 id="iptables中实现service"><a href="#iptables中实现service" class="header-anchor">#</a> iptables中实现service</h4> <ul><li>本小节所有的iptables的规则都来自于NAT表，与filter没有关联，使用的基础配置为</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>apiVersion: v1
kind: Service
metadata:
  name: my-service
  namespace: default
  labels:
    app: my-app
spec:
  type: ClusterIP
  selector:
    app: multi-container-example
  ports:
    - name: http
      protocol: TCP
      port: 9090
      targetPort: 3000
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><h5 id="clusterip-service"><a href="#clusterip-service" class="header-anchor">#</a> ClusterIP Service</h5> <p><img src="/assets/img/service-clusterip.5267c471.png" alt="service-clusterip">
该图是根据<code>iptables -nvL -nat</code>的结果根据调用链得到的</p> <ul><li><code>KUBE-SERVICES</code>是k8s的service规则的入口，每个service占用<code>KUBE-SERVICES</code>的一条，target指向具体的规则，该规则由<code>KUBE-SVC</code>开头</li> <li>本例子中为<code>Chain KUBE-SVC-I37Z43XJW6BD4TLV</code>，有两条规则，每个规则是一个EndPoint</li> <li>Endpoint使用<code>KUBE-SEP</code>开头的Chain表示，SEP是Kubernetes Service Endpoint,本例中为<code>Chain KUBE-SEP-MBYDVCBRKBCVNA25</code>和<code>Chain KUBE-SEP-ZZJ42GABN6SWXEM5</code>，此处描述了真正的Endpoint的IP:Port，此处进行DNAT，将目的IP地址换为<code>10.244.0.30:3000</code>或者<code>10.244.1.4:3000</code>就可以走正常的正常的容器间的通信流程了。</li></ul> <h5 id="nodeport-service"><a href="#nodeport-service" class="header-anchor">#</a> NodePort Service</h5> <ul><li>本例中的NodePort为32732，使用<code>netstat -an | grep 32732</code>，可以看到并没有一个端口启动着，名为NodePort但是<code>Node上并没有Port</code></li> <li><img src="/assets/img/service-nodeport.1bdc9a9e.png" alt="service-nodeport"></li> <li>从上图可以看出，它与ClusterIP的不同就是<code>KUBE-SERVICES</code>的尾部追加了一个<code>Chain KUBE-NODEPORTS</code>，里面添加了一个扩展的<code>Chain KUBE-EXT-I37Z43XJW6BD4TLV</code>，然后又重新调用了ClusterIp的Chain</li></ul> <h5 id="loadbalancer-service"><a href="#loadbalancer-service" class="header-anchor">#</a> LoadBalancer Service</h5> <ul><li>此处假定部署了一个内网负载均衡器，我<code>使用L2模式</code>安装了了一个<a href="https://docs.k0sproject.io/v1.22.2+k0s.0/examples/metallb-loadbalancer/" target="_blank" rel="noopener noreferrer">MetalLB<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <h6 id="用lay2模式执行"><a href="#用lay2模式执行" class="header-anchor">#</a> 用Lay2模式执行</h6> <ul><li>将上述里的服务类型改为LoadBalancer，然后执行 <code>kubectrl get svc -n default</code>，service赋予了可被外部访问的Ip <code>192.168.40.60</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>my-service   LoadBalancer   10.102.115.213   192.168.40.60   9090:32732/TCP   47m
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li><img src="/assets/img/servce-lb.d47fc1cb.png" alt="servce-lb"></li> <li>从上图可以看出，它与ClusterIP的不同就是<code>KUBE-SERVICES</code>加了一条目的地为<code>192.168.40.60</code>(服务的对外地址)都执行<code>Chain KUBE-EXT-I37Z43XJW6BD4TLV</code>，这一条在NodePort模式下也是有的，然后就走后续流程了。</li> <li><code>ifconfig | grep 192.168.40.60</code>发现网卡上并没有该IP，所以这个IP地址也只在iptables中存在，那么问题来了，如果只在iptables存在，谁在对192.168.40.60做ARP的回应呢？</li> <li>测试一下，<code>telnet 192.168.40.60 9090</code>，然后<code>arp -an</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>...
? (192.168.40.60) at ea:d1:d9:01:e8:3d [ether] on eth0
...
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这个IP是有MAC地址的，奥秘就是metallb的Speaker在在对外喊话,<code>192.168.40.60在我这，在我这，都到我这来</code></p> <ul><li>这里有个巧妙的地方，metallb不仅玩&quot;无中生有&quot;，还玩&quot;移花接木&quot;，将虚拟出来的IP用speaker所在主机的MAC地址进行回应。 <code>ifconfig eth0</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 192.168.40.36  netmask 255.255.0.0  broadcast 192.168.255.255
        ...
        ether ea:d1:d9:01:e8:3d  txqueuelen 1000  (Ethernet)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>可以看到MAC地址192.168.40.36与192.168.40.60的相同，<code>metabllb将宿主机的MAC地址当做它虚拟的IP地址来使用了</code></p> <ul><li>确实只有一台机器的iptables做了9090端口的处理</li></ul> <h6 id="切换为bgp模式执行"><a href="#切换为bgp模式执行" class="header-anchor">#</a> 切换为BGP模式执行</h6> <ul><li>我们重点关注网络部分的实现，service在iptables的实现与Lay2模式下一样</li> <li>这个时候就无法生成ARP记录了，<code>ping 192.168.40.60</code>之后<code>arp -an</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>? (192.168.40.60) at &lt;incomplete&gt; on eth0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>无法生成记录，因为这个模式下没有ARP回应</p> <ul><li>这个时候如果要进行连通，需要添加一条路由规则</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>ip route add 192.168.40.60 via 192.168.40.36
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>这个时候<code>telnet 192.168.40.60 9090</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>Trying 192.168.40.60...
Connected to 192.168.40.60.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>就连通了</p> <ul><li>所以BGP模式其实用的是路由的方式将包丢给了宿主机。</li></ul> <h4 id="ipvs实现service"><a href="#ipvs实现service" class="header-anchor">#</a> IPVS实现service</h4> <ul><li>IPVS目前已经</li> <li>直观点，先举个例子看一下
<code>ipvsadm -Ln</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port  Scheduler Flags  -&gt; RemoteAddress:Port   Forward Weight ActiveConn InActConn
TCP  192.168.1.20:30183 rr               -&gt; 10.101.0.76:9094     Masq    1      0          0         
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p><code>kubectl get service -A | grep 30183</code></p> <div class="language- line-numbers-mode"><pre class="language-text"><code>my-data          my-external           LoadBalancer   10.102.185.160   192.168.30.50   9094:30183/TCP                  51d
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p><code>kubectl get endpoints my-external -n my-data -o jsonpath='{.subsets[0].addresses[*].ip}:{.subsets[0].ports[0].port}'</code></p> <div class="language- line-numbers-mode"><pre class="language-text"><code>10.101.0.76:9094
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>可以看到<code>192.168.1.20:30183</code>对应的是my-data空间的my-external，endpoints为<code>10.101.0.76:9094</code></p> <ul><li>对比来看，不管是ipvs还是iptables都是将service的IP:Port转发到对应的Endpoint，不过是用的模块不一样，iptables和ipvs都是基于netfilter在对应的hook上做的规则，当service的数量很多的时候，比如到一万条，iptables就是一条一条的遍历，是O(N)的，而ipvs则使用哈希表效率更高。</li></ul> <h3 id="coredns"><a href="#coredns" class="header-anchor">#</a> Coredns</h3> <ul><li>Service的命名规则如下</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>查看Pod的域名配置<code>kubectrl exec -n default multi-container-pod -c busybox-container -i -t -- cat /etc/resolv.conf</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>search default.svc.cluster.local svc.cluster.local cluster.local
nameserver 10.96.0.10
options ndots:5
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><ul><li>查看dns service <code>kubectrl get svc -n kube-system | grep dns</code></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>kube-dns         ClusterIP   10.96.0.10     &lt;none&gt;        53/UDP,53/TCP,9153/TCP   2d12h
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>Pod就是在使用CoreDNS使用的域名解析服务</li> <li><code>kubectrl exec -n default multi-container-pod -c busybox-container -i -t -- ping my-service.default.svc.cluster.local</code>和<code>kubectrl exec -n default multi-container-pod -c busybox-container -i -t -- ping my-service.default</code>都是</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>PING my-service.default (10.102.115.213): 56 data bytes
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>这样的结果。</p> <h2 id="总结"><a href="#总结" class="header-anchor">#</a> 总结</h2> <p>本文从网络的角度出发，总结和分析了容器化网络的的实现方式，有细节也有粗略，希望能够看官们提供一些帮助。如果遗漏或者可以提供更深的描述，欢迎大佬补充与评论。涉及到的内容比较多难免有所疏漏，如有错误，也望不吝指出，感谢。</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>微信公众号为“吹风的坚果”，欢迎关注，定期更新优质的计算机文章。
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h2 id="引用"><a href="#引用" class="header-anchor">#</a> 引用</h2> <ul><li><a href="https://github.com/t1anz0ng/iftree?tab=readme-ov-file" target="_blank" rel="noopener noreferrer">iftree<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://github.com/kubernetes/kubernetes/blob/master/build/pause/linux/pause.c" target="_blank" rel="noopener noreferrer">pause的代码<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://info.support.huawei.com/info-finder/encyclopedia/zh/BGP.html" target="_blank" rel="noopener noreferrer">BGP协议<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="http://www.sel.zju.edu.cn/blog/2018/03/14/%E6%8A%80%E6%9C%AF%E5%B9%B2%E8%B4%A7%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3flannel/" target="_blank" rel="noopener noreferrer">深入理解flannel<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://cloudnativelabs.github.io/post/2017-05-22-kube-pod-networking/" target="_blank" rel="noopener noreferrer">Kube-router: Kubernetes pod networking and beyond with BGP<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://cloudnativelabs.github.io/post/2017-04-18-kubernetes-networking/" target="_blank" rel="noopener noreferrer">Kubernetes Networking<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.598d8060.js" defer></script><script src="/assets/js/2.4f2e74d3.js" defer></script><script src="/assets/js/4.03d8bd1f.js" defer></script>
  </body>
</html>
