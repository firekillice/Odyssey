# 算法
## log(n)
* 幂运算的逆运算就是对数运算，加法的逆运算就是减法(幂就是不断叠加，而对数则是不断的切分)
* 所以二分查找的每次的1/2的累计其实是对1/2的幂或者说是对总数求对数的过程
* 二分查找是每次切分一般，和平衡二叉树基本保持一致，平衡二叉树也是每次切分一半

## 搜索的实质
* 不断的缩小查询范围的逼近，关键是逼近的程度，hash是一种几乎一步到位的算法

## 特征
* 有限性
* 确定性
* 可计算性

## 数据处理行为
* 排序
* 搜索
* 索引
* 分割
* 合并


## 时间复杂度
* 线性时间复杂度O(n) = O(2n) = O(3n)，因为n可以无限大，但是系数是常量，所以是线性的

## 空间复杂度
* O(1) 就是最低的空间复杂度，也就是消耗的空间与数据的规模没有关系，**O(1)并不是只要申请一个变量的意思**
* O(n) 线性空间复杂度

## 位运算
* 普通的十进制运算, +-*/
* 位运算就是二进制运算, & | ~ ^ << >>

## 算法的分解和思路
* 只考虑一个循环
* 只考虑一次逼近

## 数组与树的拓扑相等性

##  查找思路
* 数组的思维是从最大或者最小入手
* 而树的思维则是从中间入口，将问题规模缩小
* 选择合适的开始点下嘴

## 状态机
* 一般会有起始态和终止态

## 动态规划
* 与状态机的近亲性，都是某种状态转移
* 动态规划也要建立转移方程


## 一般的思路
* 起始态
* 终止态

## 算法的目的
* 在杂乱中寻找到某种关系

## 线性和树
* 多次重复的力量，线性力量
* dfs bfs的树形力量

## 搜索
* 搜索 > 动态规划 > 贪心算法; 贪心算法就是对动态规划的优化，抛弃了部分子结构的解
* 贪心算法是动态规划的一种特例，至于贪心算法能不能得到动态规划的最终值，则需要看算法的选择。

## 解空间
* 解的可能性范围

## 单调性(增减性)
* 增函数，减函数
* 当函数 f(x) 的自变量在其定义区间内增大（或减小）时，函数值f(x)也随着增大（或减小），则称该函数为在该区间上具有单调性。

## 问题收敛


## 算法的证明
* 必然解背后肯定有公式进行对应
* 一次数组遍历是在一维空间内，2次数组遍历则是在二维空间内；一维空间不能解决的问题往往在二维空间之内可以得到解决

## 树就是一种选择分支
* 所有带有决策的算法可以归结为某种数
* 叶子节点就是解空间
* 有点像将某些计算转换为多项式、或者放在解析几何中进行；比如将矩形转换为树，经某种算法转换为树

## 最终解题思路
* 搜索问题： 递归，因为有函数调度，效率较低； 动态规划，循环次数较多； 贪心算法去除了动规的一些分支，效率更高一些
* 确定主要逻辑；确定边缘逻辑；

## 树的<前中后>序遍历，使用非递归的方式
* 前中后相对的都是根节点或者说节点本身来说的
* 左手指针，右手栈，树在腰间
* 栈永远使用栈顶，指针永远指向当前最新
* 一个循环处理一个点，每pop一次，产生一个值
* 所有的数据都是经过pop之后产生的，所以最后每个节点都会入栈，也都会出栈，出栈之时就是亮相之日

## [当前值(位置、游标、前线), 缓存]的基本结构

## 单调性的问题
* 需要考虑链条反应
* 要考虑两头的反应

## 有时候考虑两者的关系，有时候考虑三者的关系；有时候有一个游标，有时候有两个游标

## 思维的肿瘤
* 以为某个计算模式一定是通用的，或者先入为主的占用了思维的资源
* 比如 树的遍历中，while(p->left) {p = p->left}，这个思维模块其实只是中序遍历的一部分查找逻辑，不能思维上将它给独立出来

## 异或
* 自己见自己消失
* 自己见0还是自己

## 大堆 小堆 都是排序堆
* 都是特殊场景需要产生的结构
* 而还有很多奇怪的场合，比如找中间某个值，第K大/小的值

## 二分查找
* 当除以2的时候，偶数的首先落到靠左的位置；奇数的时候落在中间数字
* 为1的时候是自身，为2的时候为靠左位置
* 所以1->n的情况都包含了进去

## 引用
* [算法集锦](https://zhuanlan.zhihu.com/p/105467597)